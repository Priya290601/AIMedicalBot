# AIMedicalBot
An AI-powered Retrieval-Augmented Generation (RAG) System built using Streamlit (Frontend) and Groq LLM + LangChain + FAISS (Backend AI Engine).

This application allows users to:

ğŸ“„ Load and process PDF documents
ğŸ” Convert documents into vector embeddings
ğŸ¤– Ask context-aware questions
ğŸ“š Retrieve relevant document chunks
ğŸ§  Generate accurate answers using Llama 3.3 70B
ğŸš« Prevent hallucinations (Strict context-based answers)

ğŸ—ï¸ Architecture Overview
ğŸ”¹ Frontend: Streamlit UI

The frontend is built using Streamlit, which handles:

Chat-based interface

User input (question prompt)

Displaying AI responses

Maintaining chat history (session state)

Streamlit acts as both:

ğŸ¨ UI Layer

ğŸ”„ Request Handler

ğŸ”¹ Backend: RAG AI Layer

The backend logic is handled by:

Groq LLM (Llama 3.3 70B)

LangChain Framework

FAISS Vector Database

HuggingFace Embeddings

Backend Responsibilities:

Load stored vector database

Retrieve top-k relevant chunks

Inject retrieved context into prompt

Generate structured answer

Restrict response to document context only

ğŸ”„ How Frontend is Connected to Backend

Even though this is a single Python application, it follows a logical frontend-backend separation.

Streamlit handles user interaction, while the RAG pipeline processes and generates answers.

ğŸ“š Document Processing Pipeline
ğŸ—‚ï¸ Step 1: Load PDF Files

PDFs are loaded using DirectoryLoader

Extracted using PyPDFLoader

âœ‚ï¸ Step 2: Create Text Chunks

Uses RecursiveCharacterTextSplitter

Chunk size: 500

Overlap: 50

This ensures better semantic retrieval.

ğŸ§  Step 3: Generate Embeddings

Embedding Model Used:

all-MiniLM-L6-v2

Provider: Hugging Face

Each text chunk is converted into a vector representation.

ğŸ—„ï¸ Step 4: Store in Vector Database

Vector Store Used:

FAISS

Embeddings are stored locally inside:

vectorstore/db_faiss
ğŸ’¬ Question Answering Flow
ğŸ§¾ User Query Flow

1ï¸âƒ£ User enters question in Streamlit UI
2ï¸âƒ£ Query is sent to retriever
3ï¸âƒ£ Top 3 relevant chunks are fetched
4ï¸âƒ£ Context + Question is passed to LLM
5ï¸âƒ£ Groq LLM generates structured answer
6ï¸âƒ£ Response displayed in chat UI

ğŸ§  AI Model Configuration

LLM Provider:

Groq

Model Used:

Llama 3.3 70B Versatile

Configuration:

ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.5,
    max_tokens=512
)

Capabilities:

Context-aware reasoning

Long-context handling

Deterministic responses

Reduced hallucination

ğŸ“¦ Tech Stack
Layer	Technology Used
UI	Streamlit
LLM	Groq (Llama 3.3 70B)
Framework	LangChain
Embeddings	HuggingFace
Vector DB	FAISS
Prompt Hub	LangChain Hub
Env Handling	python-dotenv
ğŸš€ How to Run the Project
1ï¸âƒ£ Clone Repository
2ï¸âƒ£ Install Dependencies
3ï¸âƒ£ Set Groq API Key

Option 1 â€“ Environment Variable (Recommended):

Mac/Linux:

export GROQ_API_KEY="your_api_key"

Windows:

set GROQ_API_KEY=your_api_key

Option 2 â€“ .env File:

GROQ_API_KEY=your_api_key
4ï¸âƒ£ Create Vector Database

Place PDFs inside data/ folder.

Run:

python create_vectorstore.py

This will:

Load PDFs

Split into chunks

Generate embeddings

Store FAISS index

5ï¸âƒ£ Run Streamlit App
streamlit run medibot.py

Open browser:

http://localhost:8501

Start chatting with your documents ğŸ‰

ğŸ”’ Prompt Engineering Strategy

The chatbot is configured to:

Use only retrieved context

Avoid hallucination

Say â€œI donâ€™t knowâ€ if answer not found

Provide direct answers (No small talk)

Custom Prompt:

Use the pieces of information provided in the context to answer user's question.
If you dont know the answer, just say that you dont know.
Dont provide anything out of the given context.
ğŸ§© Is This Really Frontend + Backend?

Yes â€” logically.

Even though it's one Python project:

Streamlit = Frontend Layer

RAG Pipeline = Backend AI Layer

This mimics real-world AI system architecture.
